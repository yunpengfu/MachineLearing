# 第2章 k-临近算法
- k-近邻（kNN, k-NearestNeighbor）算法是一种基本分类与回归方法，我们这里只讨论分类问题中的 k-近邻算法。
- k-近邻算法，采取测量不同特征值之间的距离方法进行分类。一般来说，我们只选择样本数据前k个最相似的数据，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。  
> k近邻算法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的“模型”。 k值的选择、距离度量以及分类决策规则是k近邻算法的三个基本要素。

# KNN 原理
1. 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。
2. 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。
   a. 计算新数据与样本数据集中每条数据的距离。
   b. 对求得的所有距离进行排序（从小到大，越小表示越相似）。
   c. 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。
3. 求 k 个数据中出现次数最多的分类标签作为新数据的分类。
> KNN 通俗理解：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类
## KNN 算法特点
优点：精度高、对异常值不敏感、无数据输入假定  
缺点：计算复杂度高、空间复杂度高  
适用数据范围：数值型和标称型  
## KNN 开发流程
收集数据：任何方法  
准备数据：距离计算所需要的数值，最好是结构化的数据格式  
分析数据：任何方法  
训练算法：此步骤不适用于 k-近邻算法  
测试算法：计算错误率  
使用算法：输入样本数据和结构化的输出结果，然后运行 k-近邻算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理  
## KNN 伪代码
K-邻居算法的伪代码：
1. 计算已有了类别数据集中的点与当前点之间的距离；
2. 按距离递增次序排序；
3. 选取与当前点距离最小的k个点；
4. 确定前k个点所在类别出现的频率；
5. 返回前k个点出现频率最高的类别作为当前点的预测分类。





